defaults:
  - _self_

# shared experiment settings
main:
  ex_id: 100m_tokens
  model_path: "meta-llama/Llama-3.2-1B-instruct"
  languages: ["deu_Latn", "fra_Latn", "spa_Latn", "pol_Latn", "slk_Latn", "jpn_Jpan", "cmn_Hani"]
  
## step 1 specific settings
identify_neurons: # identify neurons
  dataset_path: "HuggingFaceFW/fineweb-2"

  tokenize:
    target_num_tokens: 100_000_000
    save_dir: "data/1_output"

  record_activations:
    batch_size: 1
    chunk_size: 4096
    max_tokens: 4096 # -1 # means no limit
    recording_strategy: "grad_act"
    variant: "gate" 
    save_dir: "data/2_output"

    visualize:
      recording_strategy: "grad_act"
      save_dir: "data/2_output_visualization"

  select_neurons: # LAPE
    save_dir: "data/3_output"
    top_rate: 0.01 # Select only 1% of the neurons
    filter_rate: 0.1 # Minimum activation value to consider neuron

    filter_rate_by_strategy:
      grad_act: 0.1
      act: 0.1
    activation_bar_ratio: 0.95

  evaluate_identified_neurons:
    selected_neurons_path: null # null -> uses data/3_output/<ex_id>/lape_selected_neurons.pt
    save_dir: "data/4_output"
    eval_languages: null # null -> uses main.languages
    target_num_tokens: 131072
    seq_len: 1024
    batch_size: 1
    dataset:
      path: "HuggingFaceFW/fineweb-2"
      split: "train"
      streaming: true
      text_field: "text"
      english_path: "HuggingFaceFW/fineweb" # used only for eng_Latn
      english_name: "default"
