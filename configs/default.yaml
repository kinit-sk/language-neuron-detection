defaults:
  - _self_
env:
  HF_HOME: "/data/Projects/tomas/cache" # Set only if neede. Will override $HF_HOME


# Anchored dataset loader â€” valid everywhere
dataset_loader: &dataset_loader
  _target_: datasets.load_dataset
  path: "HuggingFaceFW/fineweb-2"
  split: "train"
  streaming: true

# shared experiment settings
main:
  ex_id: default # as a good practice, please use the same id as the config filename
  ex_name: default_spft_experiment
  description: "Default SPFT experiment configuration, contains all steps and variables, serves as template for new experiments" 
  
  model_path: "meta-llama/Llama-3.2-1B-instruct"
  languages: ["deu_Latn", "fra_Latn", "spa_Latn", "pol_Latn", "slk_Latn", "jpn_Jpan", "cmn_Hani"]
  
## step 1 specific settings
identify_neurons: # identify neurons
  run: false # global swithh to run step 1
  dataset_path: "HuggingFaceFW/fineweb-2"

  tokenize:
    run: true
    target_num_tokens: 1_000_000
    save_dir: "data/tokens"

  record_activations:
    run: true
    batch_size: 1
    chunk_size: 4096
    max_tokens: 4096 # -1 # means no limit
    variant: "gate" 
    gated_up_act_treshold: 0.1
    save_dir: "data/activations"

  select_neurons: # LAPE
    run: true
    top_rate: 0.01 # Select only 1% of the neurons
    filter_rate: 0.1 # Minimum activation value to consider neuron
    activation_bar_ratio: 0.95

  evaluate_identified_neurons:
    run: true
    selected_neurons_path: null # null -> uses data/activations/<ex_id>/lape_selected_neurons.pt
    save_dir: "outputs/identified_neurons_eval"
    eval_languages: null # null -> uses main.languages
    target_num_tokens: 131072
    seq_len: 1024
    batch_size: 1
    dataset:
      path: "HuggingFaceFW/fineweb-2"
      split: "train"
      streaming: true
      text_field: "text"
      english_path: "HuggingFaceFW/fineweb" # used only for eng_Latn
      english_name: "default"

  visualize:
    neurons_per_layer: true
    neuron_overlap: true

## evaluation settings
evaluation:
  run: false

## fine-tuning settings
fine_tuning:
  run: false
  which_config: "full_ft_llama.yaml" # config file in pipeline/finetuning/configs/
