defaults:
  - _self_
env:
  HF_HOME: "/data/Projects/tomas/cache" # Set only if neede. Will override $HF_HOME


# Anchored dataset loader â€” valid everywhere
dataset_loader: &dataset_loader
  _target_: datasets.load_dataset
  path: "HuggingFaceFW/fineweb-2"
  split: "train"
  streaming: true

# shared experiment settings
main:
  ex_id: default # as a good practice, please use the same id as the config filename
  ex_name: default_spft_experiment
  description: "Default SPFT experiment configuration, contains all steps and variables, serves as template for new experiments" 
  
  model_path: "meta-llama/Llama-3.2-1B-instruct"
  languages: ["deu_Latn", "fra_Latn", "spa_Latn", "pol_Latn", "slk_Latn", "jpn_Jpan", "cmn_Hani"]
  
## step 1 specific settings
identify_neurons: # identify neurons
  run: false # global swithh to run step 1
  dataset_path: "HuggingFaceFW/fineweb-2"

  tokenize:
    run: true
    target_num_tokens: 1_000_000
    save_dir: "data/tokens"

  record_activations:
    run: true
    batch_size: 1
    chunk_size: 4096
    max_tokens: 8192 # -1 means no limit
    variant: "gate" 
    gated_up_act_treshold: 0.1
    save_dir: "data/activations"
    visualize: true

  select_neurons: # LAPE
    run: true
    top_rate: 0.01
    filter_rate: 0.95
    activation_bar_ratio: 0.95

  visualize:
    neurons_per_layer: true
    neuron_overlap: true

## evaluation settings
evaluation:
  run: false

## fine-tuning settings
fine_tuning:
  run: false
  which_config: "full_ft_llama.yaml" # config file in pipeline/finetuning/configs/
